# ğŸªŸ Swin Transformer: Hierarchical Vision Transformer for MNIST

Welcome to the Swin Transformer playground!  
This repo contains a **from-scratch PyTorch implementation of the Swin Transformer architecture** and a notebook showing how to train it for MNIST digit classification.  
Swin Transformer is a breakthrough in vision transformers, using shifted windows and hierarchical feature extraction for scalable, efficient image understanding.

---

## ğŸ“¦ Folder Structure

```
Swin-Transformer/
â”œâ”€â”€ swin.py                # Swin Transformer model implementation
â”œâ”€â”€ swin_t.ipynb           # Swin Transformer MNIST notebook
â”œâ”€â”€ images/                # Architecture diagrams & visualizations
â”‚   â”œâ”€â”€ swin_overview.png
â”‚   â”œâ”€â”€ swin_window_attention.png
â”‚   â”œâ”€â”€ swin_shifted_windows.png
â”‚   â””â”€â”€ ... (add more as needed)
â””â”€â”€ README.md              # You're here!
```

---

## ğŸ–¼ï¸ Swin Transformer Visuals

| Diagram                                               | Description                                                                                                  |
| ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| ![Swin Overview](images/swin_overview.png)            | **Swin Transformer Architecture:** Shows the hierarchical stages, patch merging, and window attention.       |
| ![Window Attention](images/swin_window_attention.png) | **Window-based Self-Attention:** Visualizes how attention is computed within local windows.                  |
| ![Shifted Windows](images/swin_shifted_windows.png)   | **Shifted Windows:** Demonstrates how windows are shifted between blocks to enable cross-window connections. |

---

## ğŸ¤” What is Swin Transformer?

Swin Transformer (Liu et al., 2021) is a **hierarchical vision transformer** designed for scalable image modeling.  
Its key innovations:

- **Window-based Self-Attention:**

  - Instead of global attention, Swin computes self-attention within non-overlapping windows, making it efficient for large images.

- **Shifted Windows:**

  - Every other block, windows are shifted by half their size, allowing information to flow across window boundaries and improving modeling power.

- **Hierarchical Feature Maps:**

  - The model downsamples feature maps between stages (patch merging), similar to CNNs, enabling multi-scale representation.

- **Flexible & Scalable:**
  - Works for classification, detection, segmentation, and more.

---

## ğŸ› ï¸ Implementation Details

- **CyclicShift:** Efficiently shifts windows for cross-window attention.
- **WindowAttention:** Computes self-attention within each window, with optional relative positional embeddings and masking for shifted windows.
- **PatchMerging:** Downsamples feature maps between stages, doubling channels.
- **StageModule:** Stacks Swin blocks (regular and shifted) for each stage.
- **SwinTransformer:** Four stages, each with its own depth, heads, and downscaling factor, ending with a classification head.

**Original Paper Reference:**  
Liu et al., 2021 â€” [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)

---

## ğŸƒâ€â™‚ï¸ Quickstart

**Install dependencies:**

```bash
pip install torch torchvision matplotlib einops numpy
```

**Try the Swin Transformer on MNIST:**

- See `swin_t.ipynb` for the full training notebook.
- Adapts Swin for MNIST by setting `channels=1`, `window_size=4`, and appropriate downscaling factors.
- Plots training loss and accuracy.

---

## ğŸ“Š Results

- **MNIST Classification:**
  - Swin Transformer achieves strong accuracy, showing its flexibility beyond large-scale vision tasks.
  - Training curves and validation accuracy are plotted in the notebook.

---

## ğŸ’¬ Interactive Exploration

- Play with the number of layers, heads, or window size in `swin.py`.
- Visualize attention maps or feature maps (add images to `images/`).
- Try Swin on other datasetsâ€”CIFAR, Fashion MNIST, etc.

---

## ğŸ“š References

- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Liu et al., 2021)](https://arxiv.org/abs/2103.14030)

---

## ğŸ™Œ Contributing & Feedback

Questions, suggestions, or want to add your own diagrams?  
Open an issue or pull requestâ€”everyoneâ€™s welcome!

---

**Happy windowing and classifying! ğŸªŸğŸ–¼ï¸**
